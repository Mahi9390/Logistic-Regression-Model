# -*- coding: utf-8 -*-
"""DS_Assignment_6.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sJrUzRv6EVPsZNnjtrQ_TJUvWFJ9c40x

# Data Exploration
"""

import pandas as pd
import  numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import  warnings
warnings.filterwarnings('ignore')

df=pd.read_csv('Titanic_train.csv')
df.head(30)

df.info()

df.describe()

df.isnull().sum()

df['Age'].fillna(df['Age'].mean(),inplace=True)
df.isnull().sum()

df.describe()

sns.pairplot(df, hue='Survived')
plt.show()



df.hist(figsize=(10, 8))
plt.tight_layout()
plt.show()

sns.countplot(x='Pclass', hue='Survived', data=df)
plt.title('Survival Count by Passenger Class')
plt.show()

numerical_cols = df.select_dtypes(include=np.number).columns
for col in numerical_cols:
    if col != 'PassengerId': # PassengerId is an identifier and not a feature for outlier detection
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1

        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
        print(f"Number of outliers in {col}: {len(outliers)}")



sns.countplot(x='Sex', hue='Survived', data=df)
plt.title('Survival Count by Sex')
plt.show()

sns.countplot(x='Embarked', hue='Survived', data=df)
plt.title('Survival Count by Embarked')
plt.show()

sns.kdeplot(data=df, x='Age', hue='Survived', fill=True)
plt.title('Density Plot of Age by Survived')
plt.show()

"""# Data Preprocessing"""

df.drop('PassengerId', axis=1, inplace=True)
df.head()

df['Cabin'].fillna(df['Cabin'].mode()[0],inplace=True)

df['Embarked'].fillna(df['Embarked'].mode()[0],inplace=True)
df.isnull().sum()

categorical_cols = df.select_dtypes(include=['object', 'category']).columns
# Ensure 'Sex' is included in categorical columns
if 'Sex' not in categorical_cols:
    categorical_cols = categorical_cols.tolist() + ['Sex']

print("Categorical variables to encode:")
print(categorical_cols)

df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)
display(df_encoded.head())

"""# Model Building"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

y_train = df_encoded['Survived']
X_train = df_encoded.drop('Survived', axis=1)

print("Shape of X_train:", X_train.shape)
print("Shape of y_train:", y_train.shape)

X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

print("Shape of X_train_split:", X_train_split.shape)
print("Shape of X_test_split:", X_test_split.shape)
print("Shape of y_train_split:", y_train_split.shape)
print("Shape of y_test_split:", y_test_split.shape)

model = LogisticRegression()
model.fit(X_train_split, y_train_split)

y_pred = model.predict(X_test_split)
y_pred

accuracy = accuracy_score(y_test_split, y_pred)
conf_matrix = confusion_matrix(y_test_split, y_pred)
class_report = classification_report(y_test_split, y_pred)

print(f"Accuracy Score: {accuracy}")
print("\nConfusion Matrix:")
print(conf_matrix)
print("\nClassification Report:")
print(class_report)

"""# Model Evaluation"""

from sklearn.metrics import roc_curve, roc_auc_score

# Calculate evaluation metrics
accuracy = accuracy_score(y_test_split, y_pred)
conf_matrix = confusion_matrix(y_test_split, y_pred)
class_report = classification_report(y_test_split, y_pred)
roc_auc = roc_auc_score(y_test_split, model.predict_proba(X_test_split)[:, 1])


print(f"Accuracy Score: {accuracy}")
print("\nConfusion Matrix:")
print(conf_matrix)
print("\nClassification Report:")
print(class_report)
print(f"\nROC AUC Score: {roc_auc}")

# Visualize the ROC curve
fpr, tpr, thresholds = roc_curve(y_test_split, model.predict_proba(X_test_split)[:, 1])

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='red', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend()
plt.grid(True)
plt.show()

"""# Interpretation:"""

coefficients = model.coef_[0]
feature_names = X_train.columns

# Create a DataFrame for easier interpretation
coef_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})

# Sort by the absolute value of the coefficient to see the most influential features
coef_df['Abs_Coefficient'] = abs(coef_df['Coefficient'])
coef_df = coef_df.sort_values(by='Abs_Coefficient', ascending=False)

display(coef_df.head(20))

"""**Interpreting the Coefficients:**

In logistic regression, the coefficients represent the change in the *log-odds* of the dependent variable (in this case, 'Survived') for a one-unit increase in the independent variable, assuming all other variables are held constant.

*   **Positive Coefficient:** A positive coefficient indicates that as the feature value increases, the log-odds of survival increase, making survival more likely.
*   **Negative Coefficient:** A negative coefficient indicates that as the feature value increases, the log-odds of survival decrease, making survival less likely.

Keep in mind that due to the one-hot encoding of categorical variables, the coefficients for these features are relative to the dropped category (the first category alphabetically for each original categorical column when `drop_first=True` was used).





**Based on the logistic regression coefficients we just calculated**, we can discuss the significance of different features in predicting survival probability.

The magnitude of the absolute coefficient value indicates how much a one-unit change in that feature impacts the log-odds of survival, holding other features constant. A larger absolute value suggests a stronger influence on the prediction.

From the coef_df we displayed, the top features with the largest absolute coefficients are:

Sex_male: This has the largest absolute coefficient, which is negative. This indicates that being male significantly decreases the log-odds of survival compared to the reference category (female, since drop_first=True was used). This aligns with the earlier countplot showing a higher survival rate for females.
Ticket_...: Several ticket features appear in the top influential features. This suggests that certain ticket types or numbers might be strongly associated with survival. This could be due to factors like class, location on the ship, or even who purchased the tickets.
Pclass: This has a negative coefficient, indicating that being in a higher passenger class (lower Pclass value) increases the log-odds of survival. This also aligns with the earlier countplot showing higher survival rates in Pclass 1 and 2 compared to Pclass 3.
Cabin_...: Several cabin features also appear as significant. This is likely related to the location of the cabin on the ship and its proximity to lifeboats or the upper decks, which would influence survival chances.
It's important to note that while these coefficients indicate the strength of the linear relationship between the feature and the log-odds of survival in this model, they don't necessarily imply causation. However, they do provide valuable insights into which features the logistic regression model is leveraging most heavily to make predictions.

# Testing on External Data
"""

# Load the test dataset
df_test = pd.read_csv('Titanic_test.csv')
display(df_test.head())

# Store PassengerIds for submission
passenger_ids = df_test['PassengerId']

# Preprocess the test data: fill missing values and encode categorical features
df_test_processed = df_test.copy()

# Fill missing 'Age' values with the mean of the training data's Age
# Use the mean from the original training data before any splits
df_test_processed['Age'].fillna(df['Age'].mean(), inplace=True)

# Fill missing 'Fare' values with the mean of the training data's Fare
# Use the mean from the original training data before any splits
df_test_processed['Fare'].fillna(df['Fare'].mean(), inplace=True)

# Fill missing 'Cabin' values with the mode of the training data's Cabin
df_test_processed['Cabin'].fillna(df['Cabin'].mode()[0], inplace=True)

# Fill missing 'Embarked' values with the mode of the training data's Embarked
df_test_processed['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)

# Drop 'PassengerId' and 'Name' as they are not needed for the model
df_test_processed.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)

# Select categorical columns for encoding, ensure 'Sex' is included
categorical_cols_test = df_test_processed.select_dtypes(include=['object', 'category']).columns
if 'Sex' not in categorical_cols_test:
    categorical_cols_test = categorical_cols_test.tolist() + ['Sex']

df_test_encoded = pd.get_dummies(df_test_processed, columns=categorical_cols_test, drop_first=True)

display(df_test_encoded.head())
display(df_test_encoded.isnull().sum())

# Align columns - crucial for prediction
X_train_cols = X_train.columns
X_test_encoded_aligned = df_test_encoded.reindex(columns=X_train_cols, fill_value=0)

# Verify columns match
print("Columns in training data (X_train):")
print(X_train.columns)
print("\nColumns in processed and aligned test data (X_test_encoded_aligned):")
print(X_test_encoded_aligned.columns)

# Verify the shapes are consistent
print("\nShape of X_train:", X_train.shape)
print("Shape of X_test_encoded_aligned:", X_test_encoded_aligned.shape)

# Make predictions on the test data
predictions = model.predict(X_test_encoded_aligned)

# Display the predictions
print("Predictions on the test data:")
print(predictions)

# Create the submission DataFrame
submission_df = pd.DataFrame({'PassengerId': passenger_ids, 'Survived': predictions})

# Display the first few rows of the submission DataFrame
display(submission_df.head())

# Save the submission DataFrame to a CSV file
submission_df.to_csv('submission.csv', index=False)

print("\nSubmission file 'submission.csv' created successfully!")

"""# Task
Deploy the trained logistic regression model to Streamlit.

## Save the trained model

### Subtask:
Save the trained logistic regression model to a file using a library like `pickle` or `joblib`.

**Reasoning**:
Save the trained logistic regression model to a file using joblib.
"""

import joblib

joblib.dump(model, 'logistic_regression_model.pkl')

"""## Create a streamlit application script

### Subtask:
Create a new Python script that will serve as your Streamlit application.

**Reasoning**:
Create a new Python script for the Streamlit application and add the necessary imports, title, and description.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import joblib
# import numpy as np
# 
# st.title('Titanic Survival Prediction')
# st.write('Predict the survival probability of a passenger on the Titanic.')

"""## Load the model and make predictions

### Subtask:
In the Streamlit script, load the saved model and create a user interface for inputting data. Use the loaded model to make predictions based on the user's input.

**Reasoning**:
Load the saved model, define the expected column order, and create the Streamlit UI for user input, including aligning the input data with the training columns before making a prediction and displaying it.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import joblib
# import numpy as np
# 
# st.title('Titanic Survival Prediction')
# st.write('Predict the survival probability of a passenger on the Titanic.')
# 
# # Load the trained logistic regression model
# model = joblib.load('logistic_regression_model.pkl')
# 
# # Define the expected column order from the training data
# # Assuming X_train was the DataFrame used to train the model
# # We need to load the training data or have access to its columns
# # For this example, let's assume we have the column names saved or can recreate them
# # based on the preprocessing steps. A more robust solution would save the column list.
# # Based on the previous steps, the columns after one-hot encoding included original
# # numerical features, the 'Sex_male' column, and one-hot encoded 'Name', 'Ticket',
# # 'Cabin', and 'Embarked' columns (dropping the first category).
# 
# # Let's assume we have access to the training columns from the previous run
# # In a real scenario, you would save these columns during training.
# # For demonstration, we will load the training data again to get the columns.
# # This is not ideal for deployment, but serves the purpose here.
# # A better approach is to save the list of columns during the training phase.
# 
# # Load the training data to get the column names
# df_train_for_cols = pd.read_csv('Titanic_train.csv')
# df_train_for_cols['Age'].fillna(df_train_for_cols['Age'].mean(), inplace=True)
# df_train_for_cols['Cabin'].fillna(df_train_for_cols['Cabin'].mode()[0],inplace=True)
# df_train_for_cols['Embarked'].fillna(df_train_for_cols['Embarked'].mode()[0],inplace=True)
# df_train_for_cols.drop('PassengerId', axis=1, inplace=True)
# categorical_cols_train = df_train_for_cols.select_dtypes(include=['object', 'category']).columns
# if 'Sex' not in categorical_cols_train:
#     categorical_cols_train = categorical_cols_train.tolist() + ['Sex']
# df_encoded_train_for_cols = pd.get_dummies(df_train_for_cols, columns=categorical_cols_train, drop_first=True)
# training_columns = df_encoded_train_for_cols.drop('Survived', axis=1).columns.tolist()
# 
# 
# # Create input fields for user data
# st.sidebar.header('Passenger Data Input')
# 
# pclass = st.sidebar.selectbox('Passenger Class', [1, 2, 3])
# sex = st.sidebar.selectbox('Sex', ['female', 'male'])
# age = st.sidebar.slider('Age', 0.42, 80.0, 29.7) # Using min, max, and mean from df.describe()
# sibsp = st.sidebar.number_input('Number of Siblings/Spouses Aboard', 0, 8, 0) # Using min, max from df.describe()
# parch = st.sidebar.number_input('Number of Parents/Children Aboard', 0, 6, 0) # Using min, max from df.describe()
# fare = st.sidebar.slider('Fare', 0.0, 512.33, 32.20) # Using min, max, and mean from df.describe()
# embarked = st.sidebar.selectbox('Port of Embarkation', ['C', 'Q', 'S'])
# cabin = st.sidebar.text_input('Cabin (Optional)') # Cabin can be complex, treat as text for now or simplify
# 
# # Create a button to trigger prediction
# if st.button('Predict Survival'):
#     # Create a dictionary from user inputs
#     user_input = {
#         'Pclass': pclass,
#         'Sex': sex,
#         'Age': age,
#         'SibSp': sibsp,
#         'Parch': parch,
#         'Fare': fare,
#         'Embarked': embarked,
#         'Cabin': cabin if cabin else 'B96 B98' # Use mode if not provided
#     }
# 
#     # Create a DataFrame from user input
#     input_df = pd.DataFrame([user_input])
# 
#     # Preprocess user input to match training data format
# 
#     # Handle categorical features using one-hot encoding
#     categorical_cols_input = ['Sex', 'Embarked', 'Cabin'] # Specify categorical columns for input
#     input_df_encoded = pd.get_dummies(input_df, columns=categorical_cols_input, drop_first=True)
# 
#     # Align columns with the training data columns
#     input_df_aligned = input_df_encoded.reindex(columns=training_columns, fill_value=0)
# 
#     # Make prediction
#     prediction = model.predict(input_df_aligned)
# 
#     # Display the prediction
#     if prediction[0] == 1:
#         st.success('The passenger is predicted to survive!')
#     else:
#         st.error('The passenger is predicted not to survive.')
#

"""## Display the predictions

### Subtask:
Display the model's predictions in the Streamlit application.

## Run the streamlit application

### Subtask:
Provide instructions on how to run the Streamlit application locally.

## Summary:

## Data Analysis Key Findings

*   The trained logistic regression model was successfully saved to a file named `logistic_regression_model.pkl` using the `joblib` library.
*   A Streamlit application script (`app.py`) was created, initialized with necessary imports and a title/description.
*   The `app.py` script was updated to load the saved logistic regression model and include a user interface in the sidebar for inputting passenger data (Passenger Class, Sex, Age, SibSp, Parch, Fare, Embarked, and Cabin).
*   The application preprocesses the user input, aligns the columns with the training data format, and uses the loaded model to make a survival prediction when the "Predict Survival" button is clicked.
*   The prediction result (Survived or Not Survived) is displayed to the user using Streamlit's success and error indicators.

## Insights or Next Steps

*   For a more robust deployment, save the list of training columns during the model training phase and load them in the Streamlit app instead of reloading and reprocessing the training data to get the column names.
*   Consider adding input validation and error handling to the Streamlit app to provide better feedback to the user for invalid inputs.
"""

